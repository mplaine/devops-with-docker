Notes
=====


Part 0: General Information
---------------------------
* Containers are a lightweight, portable way to package and deploy software applications.
* Confirm that Docker installed correctly by opening a terminal and running:
  docker -v
* Git guide: https://guides.github.com/activities/hello-world/
* Script command: https://linux.die.net/man/1/script


Part 1: DevOps with Docker
--------------------------
* In DevOps, Dev refers to the development of software and Ops to operations.
* Simple definition for DevOps would be that it means the release, configuring, and monitoring of software is in the hands of the very people who develop it.
* Formal definition of DevOps by Jabbari et al.: https://dl.acm.org/citation.cfm?id=2962707
  - "DevOps is a development methodology aimed at bridging the gap between Development and Operations, emphasizing communication and collaboration, continuous integration, quality assurance and delivery with automated deployment utilizing a set of development practices."
* Stack Overflow 2023 Developer Survey: https://survey.stackoverflow.co/2023/
* Work salary and experience by developer type: https://insights.stackoverflow.com/survey/2020#work-salary-and-experience-by-developer-type
* How technologies are connected: https://survey.stackoverflow.co/2020#technology-how-technologies-are-connected
* Docker is a set of platform as a service (PaaS) products that use OS-level virtualization to deliver software in packages called containers.
  - Docker is a set of tools to deliver software in containers.
  - Containers are packages of software.
* Containers are isolated so that they don't interfere with each other or the software running outside of the containers.
* Benefits from containers:
  1. works on all machines
  2. isolated environments
  3. quickly set up development environments
  4. scalable applications
* Virtual Machines are not the same as Containers - they solve different problems.
* Containers have a direct access to your own Operating Systems kernel and resources.
* Docker uses Linux kernels.
* Containers are instances of images.
* Docker image provides all the necessary instructions and dependencies for the container to run, just like a recipe provides the steps and ingredients to make a meal.
* In short, an image is like a blueprint or template, while a container is an instance of that blueprint or template.
* A Docker image is immutable, they cannot be changed after they are created.
* This image file is built from an instructional file named Dockerfile (default name) that is parsed when you run docker image build.
* Dockerfile is the instruction set for building an image.
* We're now 2 recipes deep, as Dockerfile is the recipe for an image and an image is the recipe for a container.
* Containers only contain that which is required to execute an application; and you can start, stop and interact with them. They are isolated environments in the host machine with the ability to interact with each other and the host machine itself via defined methods (TCP/UDP).
* "Docker Engine" that is made up of 3 parts: CLI (frontend), a REST API and Docker daemon (backend).
* CLI commands: https://docs.docker.com/engine/reference/commandline/cli/
* Before removing images, you should have the referencing container removed first.
* Containers can be referenced by their ID or name
  - Notice that the command also works with the first few characters of an ID. For example, docker container rm 3d4b
* Dangling images are images that do not have a name and are not used.
* The -d flag starts a container detached, meaning that it runs in the background.
* The -t flag will create a tty when running containers.
* The -i flag will instruct to pass the STDIN to the container.
* Interact with the container by using the command line:
  docker run -it ubuntu
* Useful flags when running containers:
  -i (interactive)
  -t (tty)
  -d (detached)
  --rm (Automatically remove the container when it exits)
  --name <CONTAINER_NAME>
* Attach allows the attached window to control the running container from another terminal window.
* If we want to attach to a container while making sure we don't close (stop) it from the other terminal we can specify to not attach STDIN with --no-stdin option.
* See process IDs:
  ps aux
* Exit the container:
  exit
* Emulated execution may be less efficient in terms of performance than running the image on a compatible native processor architecture.
* Quite a few popular images are so-called multi platform images (https://docs.docker.com/build/building/multi-platform/), which means that one image contains variations for different architectures. When you are about to pull or run such an image, Docker will detect the host architecture and give you the correct type of image.
* An image like Ubuntu contains already a nice set of tools but sometimes just the one that we need is not within the standard distribution. We can install any software in the container by using apt-get (https://help.ubuntu.com/community/AptGet/Howto):
  apt-get update
  apt-get -y install nano
* Installing a program or library to a container happens just like the installation is done in "normal" Ubuntu.
  - The installation of Nano is not permanent, that is, if we remove our container, all is gone.
* Connect to a running container and run processes inside it using exec.
* Share the container "session" with attach.
* Images are the basic building blocks for containers and other images.
* When running a command such as docker run hello-world, Docker will automatically search Docker Hub for the image if it is not found locally.
* Search for images in the Docker Hub with docker search.
*  Official images (https://docs.docker.com/docker-hub/official_images/) are curated and reviewed by Docker, Inc. and are usually actively maintained by the authors. They are built from repositories in the docker-library (https://github.com/docker-library).
* Automated images means that the image is automatically built (https://docs.docker.com/docker-hub/builds/) from the source repository. Its Docker Hub page (https://hub.docker.com/r/tutum/hello-world/) shows its previous "Builds" and a link to the image's "Source Repository" (in this case, to GitHub) from which Docker Hub builds the image.
* There are also other Docker registries competing with Docker Hub, such as Quay (https://quay.io/).
* By default, docker search will only search from Docker Hub, but to a search different registry, you can add the registry address before the search term, for example, docker search quay.io/hello. Alternatively, you can use the registry's web pages to search for images.
* So, if the host's name (here: quay.io) is omitted, it will pull from Docker Hub by default.
  docker pull quay.io/nordstrom/hello-world:2.0
* Since we didn't specify a tag, Docker defaulted to latest, which is usually the latest image built and pushed to the registry. However, in this case, the repository's README (https://hub.docker.com/_/ubuntu) says that the ubuntu:latest tag points to the "latest LTS" instead since that's the version recommended for general use.
* Images can be tagged to save different versions of the same image. You define an image's tag by adding :<tag> after the image's name.
* Images are composed of different layers that are downloaded in parallel to speed up the download.
* We can also tag images locally for convenience, for example, docker tag ubuntu:22.04 ubuntu:jammy_jellyfish creates the tag ubuntu:jammy_jellyfish which refers to ubuntu:22.04.
  - Can be used for renaming images.
* An image name may consist of 3 parts plus a tag. Usually like the following: registry/organisation/image:tag. But may be as short as ubuntu, then the registry will default to Docker hub, organisation to library and tag to latest. The organisation may also be a user, but calling it an organisation may be more clear.
* Docker Hub: https://hub.docker.com/
* Dockerfile is simply a file that contains the build instructions for an image.
* Alpine (https://www.alpinelinux.org/) is a small Linux distribution that is often used to create small images.
* Ubuntu images by default contain more tools to debug what is wrong when something doesn't work.
* We will choose exactly which version of a given image we want to use instead of "latest". This guarantees that we don't accidentally update through a breaking change, and we know which images need updating when there are known security vulnerabilities in old images.
* We can use the command docker build (https://docs.docker.com/engine/reference/commandline/build/) to turn the Dockerfile to an image.
* By default docker build will look for a file named Dockerfile. Now we can run docker build with instructions where to build (.) and give it a name (-t <name>).
  docker build . -t hello-docker
* The steps here represent layers (https://docs.docker.com/build/guide/layers/) of the image so that each step is a new layer on top of the base image (alpine:3.19 in our case).
* Layers have multiple functions. We often try to limit the number of layers to save on storage space but layers can work as a cache during build time. If we just edit the last lines of Dockerfile the build command can start from the previous layer and skip straight to the section that has changed.
* COPY automatically detects changes in the files, so if we change the hello.sh it'll run from step 3/3, skipping 1 and 2. This can be used to create faster build pipelines.
* It is also possible to manually create new layers on top of a image.
* Use the command docker diff (https://docs.docker.com/reference/cli/docker/container/diff/) to check what has changed
* Check what has changed in a container:
  docker diff <CONTAINER>
* The character in front of the file name indicates the type of the change in the container's filesystem: A = added, D = deleted, C = changed.
* The command docker commit added a new layer on top of the image hello-docker, and the resulting image was given the name hello-docker-additional.
* Defining the changes to the Dockerfile is much more sustainable method of managing changes than using "docker commit", and thus the latter should be avoided.
* CMD is executed when we call docker run, unless we overwrite it.
* The Docker documentation CMD says a bit indirectly that if a image has ENTRYPOINT defined, CMD is used to define it the default arguments.
* Build a Docker image from a Dockerfile with a specific name:
  docker build -f Dockerfile . -t web-server
* Running a container and defining start conditions for it (instead of adding instructions one by one to Dockerfile and fails after each addition):
  docker run -it ubuntu:22.04
  apt-get update && apt-get install -y curl
  curl -L https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp -o /usr/local/bin/yt-dlp
  chmod a+rx /usr/local/bin/yt-dlp
  apt-get install -y python3
  yt-dlp
* So now when we know exactly what we need. Starting FROM ubuntu:22.04, we'll add the above steps to our Dockerfile. We should always try to keep the most prone to change rows at the bottom, by adding the instructions to the bottom we can preserve our cached layers - this is a handy practice to speed up the build process when there are time-consuming operations like downloads in the Dockerfile.
* Override bash as our image command (set on the base image) with yt-dlp itself:
  FROM ubuntu:22.04
  # ...
  CMD ["/usr/local/bin/yt-dlp"]
* Given arguments replace the command or CMD (not working):
  docker run yt-dlp https://www.youtube.com/watch?v=uTZSILGTskA
* We can use ENTRYPOINT (https://docs.docker.com/engine/reference/builder/#entrypoint) to define the main executable and then Docker will combine our run arguments for it.
* With ENTRYPOINT docker run now executed the combined /usr/local/bin/yt-dlp https://www.youtube.com/watch?v=uTZSILGTskA inside the container!
* ENTRYPOINT vs CMD can be confusing - in a properly set up image, such as our yt-dlp, the command represents an argument list for the entrypoint. By default, the entrypoint in Docker is set as /bin/sh -c and this is passed if no entrypoint is set. This is why giving the path to a script file as CMD works: you're giving the file as a parameter to /bin/sh -c.
* If an image defines both, then the CMD is used to give default arguments (https://docs.docker.com/engine/reference/builder/#cmd) to the entrypoint.
* The argument defined by CMD can be overridden by giving one in the command line:
  docker run yt-dlp https://www.youtube.com/watch?v=DptFY_MszQs
* In addition to all seen, there are two ways to set the ENTRYPOINT and CMD: exec form and shell form. We've been using the exec form where the command itself is executed. In shell form the command that is executed is wrapped with /bin/sh -c - it's useful when you need to evaluate environment variables in the command like $MYSQL_PASSWORD or similar.
* In the shell form, the command is provided as a string without brackets. In the exec form the command and it's arguments are provided as a list (with brackets).
* In summary, ENTRYPOINT and CMD will be prefixed with /bin/sh -c when used without brackets.
  Dockerfile                         | Resulting command
  -------------------------------------------------------------------------------------
  ENTRYPOINT /bin/ping -c 3          | /bin/sh -c '/bin/ping -c 3' /bin/sh -c localhost
  CMD localhost                      | 
  -------------------------------------------------------------------------------------
  ENTRYPOINT ["/bin/ping","-c","3"]  | /bin/ping -c 3 /bin/sh -c localhost
  CMD localhost                      | 
  -------------------------------------------------------------------------------------
  ENTRYPOINT /bin/ping -c 3          | /bin/sh -c '/bin/ping -c 3' localhost
  CMD ["localhost"]                  | 
  -------------------------------------------------------------------------------------
  ENTRYPOINT ["/bin/ping","-c","3"]  | /bin/ping -c 3 localhost
  CMD ["localhost"]                  | 
  -------------------------------------------------------------------------------------
* Most of the time we can ignore ENTRYPOINT when building our images and only use CMD. For example, Ubuntu image defaults the ENTRYPOINT to bash so we do not have to worry about it.
* Copy the file from the container to the host machine using docker cp:
  docker cp "upbeat_pascal://mydir/Welcome to Kumpula campus! ｜ University of Helsinki [DptFY_MszQs].mp4" .
* We can use Docker volumes (https://docs.docker.com/storage/volumes/) to make it easier to store the downloads outside the container's ephemeral storage. With bind mount (https://docs.docker.com/storage/bind-mounts/) we can mount a file or directory from our own machine (the host machine) into the container.
* The -v option requires an absolute path:
  docker run -v "$(pwd):/mydir" yt-dlp "https://www.youtube.com/watch?v=DptFY_MszQs"
* A volume is simply a folder (or a file) that is shared between the host machine and the container.
* If we wish to create a volume with only a single file we could also do that by pointing to it. For example -v "$(pwd)/material.md:/mydir/material.md" this way we could edit the material.md locally and have it change in the container (and vice versa). Note also that -v creates a directory if the file does not exist.
* Sending messages: Programs can send messages to URL addresses such as this: http://127.0.0.1:3000 where HTTP is the protocol, 127.0.0.1 is an IP address, and 3000 is a port. Note the IP part could also be a hostname: 127.0.0.1 is also called localhost so instead you could use http://localhost:3000.
* Receiving messages: Programs can be assigned to listen to any available port. If a program is listening for traffic on port 3000, and a message is sent to that port, the program will receive and possibly process it.
* The address 127.0.0.1 and hostname localhost are special ones, they refer to the machine or container itself, so if you are on a container and send a message to localhost, the target is the same container. Similarly, if you are sending the request from outside of a container to localhost, the target is your machine.
* It is possible to map your host machine port to a container port.
* Exposing a container port means telling Docker that the container listens to a certain port. This doesn't do much, except it helps humans with the configuration.
  - To expose a port, add the line EXPOSE <port> in your Dockerfile
* Publishing a port means that Docker will map host ports to the container ports.
  - To publish a port, run the container with -p <host-port>:<container-port>
  - If you leave out the host port and only specify the container port, Docker will automatically choose a free port as the host port:
    docker run -p 4567 app-in-port
* We could also limit connections to a certain protocol only, e.g. UDP by adding the protocol at the end: EXPOSE <port>/udp and -p <host-port>:<container-port>/udp
* Here we did a quick trick to separate installing dependencies from the part where we copy the source code in. The COPY will copy both files Gemfile and Gemfile.lock to the current directory. This will help us by caching the dependency layers if we ever need to make changes to the source code. The same kind of caching trick works in many other languages or frameworks, such as Node.js.
  COPY Gemfile* ./
* The environment variables set using ENV will persist when a container is run from the resulting image. You can view the values using docker inspect, and change them using docker run --env <key>=<value>.
* Docker CLI login (to authenticate our push):
  docker login
* Create an image to be pushed to Docker Hub:
  docker tag streamlit-project mplaine/streamlit-project
* Push an image to Docker Hub:
  docker push mplaine/streamlit-project
* Start recording terminal commands:
  script [options] [file]
  script exercise12_1.txt     # text output file
  script -r exercise12_1.txt  # binary output file
* Stop recording terminal commands:
  exit
* Replay recorded terminal commands:
  cat exercise12_1.txt        # text input file
  script -p exercise12_1.txt  # binary input file
* Docker commands:
  - Image:
    - List all images: docker image ls
    - Remove an image: docker image rm <IMAGE>
    - Remove unused images: docker image prune
    - Download image: docker image pull <IMAGE>
    - Create a tag <TARGET_IMAGE> that refers to <SOURCE_IMAGE>: docker image tag <SOURCE_IMAGE>[:TAG] <TARGET_IMAGE>[:TAG]
  - Container:
    - List all containers: docker container ls -a
    - List running containers: docker container ls --OR-- docker ps
    - List and filter all containers: docker container ls -a | grep <FILTER>
    - List last 3 containers: docker container ls -a --last 3
    - Run a container (download the image if it is not available): docker container run <IMAGE>
    - Run a container detached: docker container run -d <IMAGE>
    - Remove a container: docker container rm <CONTAINER>
    - Remove all stopped containers: docker container prune
    - Stop a container: docker container stop <CONTAINER>
    - Start a container: docker container start <CONTAINER>
    - Execute a command inside the container: docker container exec <CONTAINER>
    - Kill a running container: docker kill <CONTAINER>
    - Run processes inside a container: docker exec <CONTAINER> <COMMAND>
    - Execute the Bash shell in the container in interactive mode: docker exec -it <CONTAINER> bash
    - Attach local standard input, output, and error streams to a running container: docker attach <CONTAINER>
    - Attach local standard output and error streams to a running container: docker attach --no-stdin <CONTAINER>
    - Pause all processes within a container: docker pause <CONTAINER>
    - Unpause all processes within a container: docker unpause <CONTAINER>
    - Copy files/folders between a container and the local filesystem: docker cp <SRC_PATH> <CONTAINER>:<DEST_PATH>
    - Copy files/folders between a container and the local filesystem: docker cp <CONTAINER>:<SRC_PATH> <DEST_PATH>
    - Inspect changes to files or directories on a container's filesystem: docker diff <CONTAINER>
    - Create a new image from a container's changes: docker commit <CONTAINER> <NEW_IMAGE>
  - Build:
    - Build a Docker image: docker build <PATH>
  - Log:
    - Follow log output: docker logs -f <CONTAINER>
  - System:
    - Remove unused data: docker system prune
  - Search:
    - Search Docker Hub for images: docker search <KEYWORD>
* Dockerfile commands (https://docs.docker.com/reference/dockerfile/):
  - FROM: Create a new build stage from a base image.
  - WORKDIR: Change working directory.
  - COPY: Copy files and directories.
  - RUN: Execute build commands.
  - ENV: Set environment variables (persisted when a container is run from the resulting image).
  - HEALTHCHECK: Check a container's health on startup.
  - ENTRYPOINT: Specify default executable (cannot be overridden).
  - CMD: Specify default commands (can be overridden).


Part 2: DevOps with Docker: docker-compose
------------------------------------------
* Docker Compose (https://docs.docker.com/compose/) is designed to simplify running multi-container applications using a single command.
* Docker Compose file documentation: https://docs.docker.com/compose/compose-file/
* The value of the key build can be a file system path (in the example it is the current directory .) or an object with keys context and dockerfile. For details, see https://docs.docker.com/compose/compose-file/build/
* Volumes in Docker Compose are defined with the following syntax location-in-host:location-in-container. Compose can work without an absolute path (host).
* Give the container a name it will use when running with container_name. The service name can be used to run it:
  docker compose run yt-dlp-ubuntu https://imgur.com/JY5tHqr
* Defining environment variables: https://docs.docker.com/compose/environment-variables/set-environment-variables/
* In addition to starting services listed in docker-compose.yml Docker Compose automatically creates and joins both containers into a network with a DNS. Each service is named after the name given in the docker-compose.yml file. As such, containers can reference each other simply with their service names, which is different from the container name.
* Docker Compose has already taken care of creating a network and webapp can simply send a request to webapp-helper:3000, the internal DNS will translate that to the correct access and ports do not have to be published outside of the network.
* Redis (https://redis.io/) is an open-source in-memory storage, used as a distributed, in-memory key–value database, cache and message broker, with optional durability.
* Redis is quite often used as a cache (https://en.wikipedia.org/wiki/Cache_(computing)) to store data so that future requests for data can be served faster.
* It is also possible to define the network manually in a Docker Compose file. A major benefit of a manual network definition is that it makes it easy to set up a configuration where containers defined in two different Docker Compose files share a network, and can easily interact with each other.
* Defining network:
  version: "3.8"
  services:
    db:
      image: postgres:13.2-alpine
      networks:
        - database-network # Name in this Docker Compose file
  networks:
    database-network: # Name in this Docker Compose file
      name: database-network # Name that will be the actual name of the network
* This defines a network called database-network which is created with docker compose up and removed with docker compose down.
* Establishing a connection to an external network (that is, a network defined in another docker-compose.yml, or by some other means) is done as follows:
  version: "3.8"
  services:
    db:
      image: backend-image
      networks:
        - database-network
  networks:
    database-network:
      external:
        name: database-network # Must match the actual name of the network
* By default all services are added to a network called default. The default network can be configured and this makes it possible to connect to an external network by default as well:
  version: "3.8"
  services:
    db:
      image: backend-image
  networks:
    default:
      external:
        name: database-network # Must match the actual name of the network
* Scale SERVICE to NUM instances. Overrides the scale setting in the Compose file if present.
  docker compose up --scale whoami=3
* Use docker compose port to find out which ports the instances are bound to:
  docker compose port --index 1 whoami 8000  # 0.0.0.0:32769
* Curl the port:
  curl 0.0.0.0:32769  # I'm 536e11304357
* Load balancer: https://en.wikipedia.org/wiki/Load_balancing_(computing)
* For containerized local environment (or a single server) one good solution is to use https://github.com/jwilder/nginx-proxy
* We'll mount our docker.sock (https://stackoverflow.com/questions/35110146/can-anyone-explain-docker-sock, the socket that is used to communicate with the Docker Daemon, https://docs.docker.com/engine/reference/commandline/dockerd/) inside of the container in :ro read-only mode:
  version: "3.8"
  services:
    whoami:
      image: jwilder/whoami
    proxy:
      image: jwilder/nginx-proxy
      volumes:
        - /var/run/docker.sock:/tmp/docker.sock:ro
      ports:
        - 80:80
* Redmine (https://www.redmine.org/) is a project management application.
* Adminer (https://www.adminer.org/) is a graphical interface for database administration.
* Sentry (https://sentry.io/) is an application monitoring tool.
* restart: always was changed to unless-stopped, that will keep the container running unless we explicitly stop it. With always the stopped container is started after reboot, for example, see here for more.
  https://docs.docker.com/config/containers/start-containers-automatically/
* PostgreSQL, where to store data: https://github.com/docker-library/docs/blob/master/postgres/README.md#where-to-store-data
  - Mount the /var/lib/postgresql/data separately to preserve the data
* Docker storage options to persist data:
  - Bind mount (depend on directory structure and OS of the host machine): https://docs.docker.com/storage/bind-mounts/
  - Volume (managed by Docker): https://docs.docker.com/storage/volumes/
    - Compose uses the current directory as a prefix for container and volume names so that different projects don't clash (The prefix can be overridden with COMPOSE_PROJECT_NAME environment variable if needed).
* Inspect a container either using Docker Desktop or CLI:
  docker container inspect db_redmine | grep -A 5 Mounts
* Despite us NOT configuring one explicitly, an anonymous volume (randomly named) was automatically created for us.
* To get rid of unused volumes, you can use docker volume prune.
* The depends_on (https://docs.docker.com/compose/compose-file/compose-file-v3/#depends_on) declaration. This makes sure that the db service is started first. depends_on does not guarantee that the database is up, just that it is started first.
* Interact with the database: docker container exec -it db_redmine psql -U postgres
* Create backups with pg_dump: docker container exec db_redmine pg_dump -U postgres > redmine.dump
* Adminer actually assumes that the database has DNS name db so with this name selection, we did not have to specify anything. If the database has some other name, we have to pass it to adminer using an environment variable:
  adminer:
    environment:
      - ADMINER_DEFAULT_SERVER=database_server
* The benefit of a bind mount is that since you know exactly where the data is in your file system, it is easy to create backups. If the Docker managed volumes are used, the location of the data in the file system can not be controlled and that makes backups a bit less trivial...
* Volume bind mount path is automatically created and can be a relative path. For example:
  volumes:
    - ./database:/var/lib/postgresql/data
* A reverse proxy (e.g., Nginx, https://hub.docker.com/_/nginx) is a type of proxy server that retrieves resources on behalf of a client from one or more servers. These resources are then returned to the client, appearing as if they originated from the reverse proxy server itself.
  - A single point of entry (e.g., localhost:80 instead of localhost:5000 and localhost:8000)
* Important note on ports!!! The container port is there within the Docker network accessible by the other containers that are in the same network even if we do not publish anything. So publishing the ports is only for exposing ports outside the Docker network. If no direct access outside the network is not needed, then we just do not publish anything.
* Port scanner:
  docker run -it --rm --network host networkstatic/nmap localhost
* Containers are not only great in production. They can be used in development environments as well and offer several benefits.
* The principle in all development projects is to have a setup so that a new developer only needs to install Docker and clone the project code from GitHub to get started. Not a single dependency is ever installed on to host machine, Git, Docker and the text editor of choice are the only things that are needed.
  - For example, say you need MongoDB version 4.0.22 installed in port 5656. It's now an oneliner: "docker run -p 5656:27017 mongo:4.0.22" (MongoDB uses 27017 as the default port).
* NodeJS (https://nodejs.org/en/) is a cross-platform JavaScript runtime that makes it possible to run JavaScript in your machine, servers and embedded devices, among many other platforms
  - Libraries are defined in package.json and package-lock.json and installed with npm install. npm (https://www.npmjs.com/) is the Node package manager.
  - The main/entry file in this case the script is executed with npm start.
* We can rebuild the whole environment whenever we want with docker compose up --build
* When I change the line, on my host machine the application instantly notices that files have changed, thanks to volume binding in docker-compose.yml
* Redirect any request using Express:
  app.get('*', function (_req, res) {
    res.redirect('/plus?x=5&y=2')
  })
* Docker commands:
  - Volume:
    - List all olumes: docker volume ls
    - Remove unused local volumes: docker volume prune
* Dockerfile commands (https://docs.docker.com/reference/dockerfile/):
  - ENV: Set environment variables
  - VOLUME: Create volume mounts
* Docker Compose commands:
  - Build: Build or rebuild services: docker compose build
  - Down: Stop and remove containers, networks: docker compose down
  - Port: Print the public port for a port binding: docker compose port [--index 1] <SERVICE> <PRIVATE_PORT>
  - Push: Push service images: docker compose push
  - Run: Run a one-off command on a service: docker compose run <SERVICE>
  - Up: Create and start containers: docker compose up [-d] [--scale <SERVICE>=<NUM_INSTANCES>] [--build]
* docker-compose.yml commands (https://docs.docker.com/compose/compose-file/):
  version: '3.8'
  services
    <SERVICE>
      image: <USERNAME>/<REPOSITORY>
      build: <BUILD_PATH_FOR_IMAGE_ONLY>
        context: <CONTEXT_PATH>
        dockerfile: <DOCKERFILE_PATH>
      volumes:
        - <HOST_LOCATION>:<CONTAINER_LOCATION>
      container_name: <CONTAINER_NAME>
      restart: <STRATEGY>
      command: ["<COMMAND>"]
      ports:
        - <HOST_PORT>:<CONTAINER_PORT>
      environment:
        - <VARIABLE>=<VALUE>
      volumes:
        - <VOLUME_NAME>:<PATH_TO_STORE_DATA_IN_CONTAINER>
      depends_on:
        - <SERVICE>
    volumes:
      <VOLUME_NAME>:


Part 3: DevOps with Docker: security and optimization
-----------------------------------------------------
* The Alpine Linux image is much smaller than Ubuntu.
* Running the applications as root, i.e. the super user is potentially dangerous.
* The official images repository (https://github.com/docker-library/official-images) contains a library of images considered official. They are introduced into the library by regular pull request processes. The extended process for verifying an image is described in the repository README.
* For the Ubuntu image automation from the launchpad takes care of creating the PRs to docker-library and the maintainers of the official images repository verify the PRs.
* We have learned that the build processes are open and we can verify it if we have the need.
* CI/CD (https://en.wikipedia.org/wiki/CI/CD) automates much or all of the manual human intervention traditionally needed to get new code from a commit into production. With a CI/CD pipeline, development teams can make changes to code that are then automatically tested and pushed out for delivery and deployment. Get CI/CD right and downtime is minimized and code releases happen faster.
* Use GitHub Actions (https://github.com/features/actions) to build an image and push the image to Docker Hub, and then use a project called Watchtower (https://containrrr.dev/watchtower/) to automatically pull and restart the new image in the target machine.
* GitHub Actions (https://github.com/features/actions) is a continuous integration and continuous delivery (CI/CD) platform that allows you to automate your build, test, and deployment pipeline. You can create workflows that build and test commit and every pull request to your repository, or deploy merged pull requests to production.
* GitHub Actions
  - workflows: https://docs.github.com/en/actions/using-workflows
  - jobs: https://docs.github.com/en/actions/using-jobs/using-jobs-in-a-workflow
  - steps: https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#jobsjob_idsteps
  - secrets: https://docs.github.com/en/actions/security-guides/encrypted-secrets
    GitHub > Settings > Secrets and variables > Secrets
* The DOCKERHUB_TOKEN can be created in Docker Hub from MY Account > Security > New Access Token.
* GitHub Actions: workflows > jobs > steps
* Each step is a small operation or action that does its part of the whole.
  1. Check out the code from the repository
  2. Log in to Docker Hub
  3. Build the image and push it to Docker Hub
* Docker GitHub Actions: https://github.com/marketplace/actions/build-and-push-docker-images
* Watchtower (https://github.com/containrrr/watchtower) is an open-source project that automates the task of updating images.
  - For development environments (not for production)
* Watchtower will pull the source of the image (in this case Docker Hub) for changes in the containers that are running. The container that is running will be updated and automatically restarted when a new version of the image is pushed to Docker Hub. Watchtower respects tags e.g. q container using ubuntu:22.04 will not be updated unless a new version of ubuntu:22.04 is released.
* Note that now anyone with access to your Docker Hub also has access to your PC through this. If they push a malicious update to your application, Watchtower will happily download and start the updated version.
* Example Dockerfile using Watchtower:
  version: "3.8"
  services:
    watchtower:
      image: containrrr/watchtower
      environment:
        -  WATCHTOWER_POLL_INTERVAL=60 # Poll every 60 seconds
      volumes:
        - /var/run/docker.sock:/var/run/docker.sock
      container_name: watchtower
* One needs to be careful when starting Watchtower with docker compose up, since it will try to update every image running the machine. The documentation (https://containrrr.github.io/watchtower/) describes how this can be prevented.
* Deployment pipeline:
  1. Checkout code from repository
  2. Build and push image to Docker Hub
  3. Trigger Render deployment
* Shell scripting tutorial: https://www.shellscript.sh/
* The Watchtower uses a volume to docker.sock socket to access the Docker daemon of the host from the container:
  services:
  watchtower:
    image: containrrr/watchtower
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    # ...
* In practice this means that Watchtower can run commands on Docker the same way we can "command" Docker from the cli with docker ps, docker run etc.
* If we mount the docker.sock socket to a container, we can use the command docker inside the container, just like we are using it in the host terminal!
* Check which Docker registry you are currently logged in on your system:
  cat ~/.docker/config.json
* Log out from a Docker registry or cloud backend:
  docker logout [SERVER]
  docker logout [command]
* The application could, in theory, escape the container due to a bug in Docker or Linux kernel. To mitigate this security issue we will add a non-root user to our container and run our process with that user. Another option would be to map the root user to a high, non-existing user id on the host with https://docs.docker.com/engine/security/userns-remap/, and can be used in case you must use root within the container.
* Change the user with the directive USER - so all commands after this line will be executed as our new user, including the CMD and ENTRYPOINT.
* Add an user called appuser with the following command:
  RUN useradd -m appuser
* We'll see that our appuser user has no access to write to the container filesystem. This can be fixed with chown or not fix it at all, if the intented usage is to always have a /mydir mounted from the host. By mounting the directory the application works as intended.
* If we want to give the appuser permission to write inside the container, the permission change must be done when we are still executing as root, that is, before the directive USER is used to change the user:
  RUN useradd -m appuser  # create the appuser
  RUN chown appuser .     # change the owner of current dir to appuser
  USER appuser            # now we can change the user
* Security issues with the user being a root are serious for the example frontend and backend as the containers for web services are supposed to be accessible through the Internet.
* A small image size has many advantages, firstly, it takes much less time to pull a small image from the registry. Another thing is the security: the bigger your image is the larger the surface area for an attack it has.
* To build your own image, you create a Dockerfile with a simple syntax for defining the steps needed to create the image and run it. Each instruction in a Dockerfile creates a layer in the image. When you change the Dockerfile and rebuild the image, only those layers which have changed are rebuilt. This is part of what makes images so lightweight, small, and fast, when compared to other virtualization.
* Each command that is executed to the base image, forms a layer. The resulting image is the final layer, which combines the changes that all the intermediate layers contain. Each layer potentially adds something extra to the resulting image so it might be a good idea to minimize the number of layers.
* Methods to reduce container image size:
  1. Small base image
     - alpine
  2. Builder pattern
     - code -> build container (compiler, dev deps, unit tests, etc.) -> build artifact(s) (binaries, static files, bundles, transpiled code) -> runtime container (runtime env, debug/monitor, tooling)
* Advantages of small containers:
  1. Performance (time to build, push, and pull a container -> faster use in development or production)
  2. Security (less software included means less vulnerabilities)
* We could glue all RUN commands together to reduce the number of layers that are created when building the image.
* With docker image history we can see the size of each layer.
* Finally, remove everything that is not needed in the final image.
  - curl and its dependencies:
    RUN apt-get purge -y --auto-remove curl
  - apt source lists:
    RUN rm -rf /var/lib/apt/lists/*
* Alpine Linux (https://www.alpinelinux.org/) provides a popular alternative base in https://hub.docker.com/_/alpine/ for Ubuntu that is around 8 megabytes. It's based on alternative glibc implementation musl (https://musl.libc.org/) and busybox (https://www.busybox.net/) binaries, so not all software runs well (or at all) with it.
  - The package manager is apk and it can work without downloading sources (caches) first with --no-cache.
    apk add --no-cache curl python3 ca-certificates
  - For creating user the command useradd is missing, but adduser can be used instead.
    adduser -D appuser
  - Most of the package names are the same - there's a good package browser at https://pkgs.alpinelinux.org/packages
  - Possible issues with running software depending on glib -> check tha teverything works as expected
* Alpine uses musl libc (https://musl.libc.org/) instead of glibc and friends (https://www.etalabs.net/compare_libcs.html), so software will often run into issues depending on the depth of their libc requirements/assumptions.
  - For pros/cons, see https://news.ycombinator.com/item?id=10782897
  - Alpine reference: https://hub.docker.com/_/alpine/
* In general, installing the environment that is required to build and run a program inside a container can be quite a burden.
* Luckily, there are preinstalled images for many programming languages readily available on DockerHub, and instead of relying upon "manual" installation steps in a Dockerfile, it's quite often a good idea to use a pre-installed image.
* We can publish whatever tag variants we want:
  docker image tag yt-dlp:alpine-3.19 mplaine/yt-dlp:alpine-3.19
  docker image push mplaine/yt-dlp:alpine-3.19
  docker image tag yt-dlp:python-alpine mplaine/yt-dlp:python-alpine
  docker image push mplaine/yt-dlp:python-alpine
  docker image tag yt-dlp:python-alpine mplaine/yt-dlp  # latest
  docker image push mplaine/yt-dlp  # latest
* It's important to keep in mind that if not specified, the tag :latest simply refers to the most recent image that has been built and pushed, which can potentially contain any updates or changes.
* There are some differences between these types of Docker images:
  - Standard Docker Image: These images are typically based on a full Linux distribution like Ubuntu or Debian and include a wide range of pre-installed packages and dependencies. They tend to be larger in size than other types of images.
  - Alpine Linux Docker Image: Alpine Linux is a lightweight Linux distribution that is designed to be small and secure. Alpine-based Docker images are typically much smaller than standard images, as they include only the bare essentials needed to run the application.
  - Slim Docker Image: These images are similar to Alpine-based images in that they are designed to be small and efficient. However, they are not necessarily based on Alpine Linux and can use other lightweight Linux distributions like CentOS or Debian. Slim images typically include only the necessary packages and dependencies to run the application, but may still be larger than Alpine-based images.
* Use rm -rf /var/lib/apt/lists/* to reduce the size of the Docker image by removing unnecessary cached package lists after installing packages.
* Multi-stage builds are useful when you need some tools just for the build but not for the execution of the image (that is for CMD or ENTRYPOINT). This is an easy way to reduce size in some cases.
* Note the naming from Ruby to build-stage. We could also use an external image as a stage, --from=python:3.12 for example.
  FROM ruby:3 as build-stage
* As you can see, even though our Jekyll image needed Ruby during the build stage, it is considerably smaller since it only has Nginx and the static files in the resulting image.
* Often the best choice is to use a FROM scratch image as it doesn't have anything we don't explicitly add there, making it the most secure option over time.
* With (built) binary files, use FROM scratch to minimize Docker image size
* Best practices when building minimized Docker images:
  - Use Linux Alpine in builder stage
  - Use multi-staging (e.g., builder and server stages)
  - Build binary files using static linking for C libraries
  - Remove apt source lists when no longer needed
  - Use FROM scratch with binary files
  - Run the container using a non-root user
* 8 Best Practices for using Docker in Production (https://www.youtube.com/watch?v=8vXoMqWgbQQ) to improve security and optimize Docker image size:
  1. Use official Docker images as base image
     + Cleaner Dockerfile
     + Official Docker image
     + Verified and already build qith best practices
     + e.g. FROM node (not FROM ubuntu)
     - Do not use base operating system image and install packages yourself
  2. Use specific Docker image versions
     + Fixate the version
     + The more specific, the better
     + e.g. FROM node:17.0.1 (not FROM node)
     * It will use the latest tag
     * FROM node == FROM node:latest
     - You might get different docker image versions
     - Might break stuff
     - Latest tag is unpredictable
     - Do not use a random latest tag
  3. Use small-sized official images
     + Less storage space 
     + Transfer images faster
     + Minimize attach surface
     + Only bundle necessary utilities (e.g., Busybox instead of GNU Core Utils)
     + Use image based on a leaner and smaller OS distro
     + e.g. FROM node:17.0.1-alpine (not FROM node:17.0.1)
     * Linux Alpine is lightweight Linux distro, security-oriented, popular base image
     * If you don't require any specific utilities, choose leaner and smaller image
     - Full-blown operating system distros have larger image size due to packaged in system utilities
     - Many OS features your image will never use
     - More security vulnerabilities
     - Introduce unnecessary security issues from the beginning
     - Could ne based on full-blown OS
  4. Optimize caching image layers
     + Faster image building
     + Downloading only added layers
     + Order Dockerfile commands from least to most frequently changing (e.g., FROM, WORKDIR, COPY package.json, RUN install, COPY sources, CMD)
     + Take advantage of caching
     + Faster image building and fetching
     * Docker images are built based on a Dockerfile
     * Each command creates an image layer
     * Show Docker image layers: docker image history myapp:1.0
     * Docker caches each layer, saved on local filesystem
     * If nothing has changed in a layer (or any layers preceding it), it will be re-used from cache
     * Once a layer changes, all following layers are re-created as well
  5. Use .dockerignore file
     + Use .dockerignore to explicitly exclude files and folders (.git, .cache, *.md, .env)
     + Reduce image sized
     + Prevent unintended secrets exposure
     * We don't need everything inside the Docker image (auto-generated folders (like target, build), README files)
     * Create .dockerignore file in the root directory
     * List files and folders you want to ignore
     * Matching is done using Go's filepath.Match rules
  6. Make use of multi-stage builds
     + Leave everything behind you don't want in the final image
     * Build stage, runtime stage
     * You can name your stages with AS <NAME>
     * Each FROM instruction starts a new build stage
     * You can selectively copy artifacts from one stage to another
     - Test dependencies, temporary files, development tools, build tools, dependencies files (needed for installing dependencies, not for running the app) all increase image size and attack surface
  7. Use the least privileged user
     + Create a dedicated user and group
       RUN groupadd -r tom && useradd -g tom tom
     + Don't forget to set required permissions
       RUN chown -R tom:tom /app
     + Change to non-root user with USER directive
       USER tom
     * By default, Docker uses the root user
     * Few use cases, where the container needs to run as root
     * Some base images have a generic user bundled in (no need to create one yourself)
       RUN chown -R node:node /app
       USER node
     - Security bad practice
     - Container could potentially have root access on the Docker host
     - Easier privilege escalation for an attacker
  8. Scan your images for security vulnerabilities
     * docker scout cves myapp:1.0  (requires docker login)
     * Docker uses Snyk service (constantly updated database) for the vulnerability scan
     * Snyk shows the version of a library that fixes the vulnerability
     * Configure Docker Hub to automatically scan pushed images AND/OR integrate into your CI/CD pipeline
* In situations where we have more than a single host machine we cannot rely solely on Docker. However, Docker does contain other tools to help us with automatic deployment, scaling and management of dockerized applications.
* Docker swarm (https://docs.docker.com/engine/swarm/) mode is built into Docker. It turns a pool of Docker hosts into a single virtual host.
* Kubernetes (https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/) is the de facto way of orchestrating your containers in large multi-host environments. The reason being it's customizability, large community and robust features. However, the drawback is the higher learning curve compared to Docker swarm mode.
* Lightweight Kubernetes: https://k3s.io/
* Rather than maintaining one (Kubernetes cluster) yourself the most common way to use Kubernetes is by using a managed service by a cloud provider. Such as Google Kubernetes Engine (GKE) or Amazon Elastic Kubernetes Service (Amazon EKS) which are both offering some credits to get started.
* Kubernetes glossary: https://kubernetes.io/docs/reference/glossary/?fundamental=true
* Example Kubernetes diagrams: https://tsuyoshiushio.medium.com/kubernetes-in-three-diagrams-6aba8432541c
* Docker commands:
  - Image:
    - Show the history of an image: docker image history <IMAGE>
    - Show digests: docker image ls --digests
    - Show the history of an image: docker image history --no-trunc <IMAGE:TAG>
  - Scout:
    - Display CVEs identified in a software artifact: docker scout cves <IMAGE:TAG>
  - Swarm:
    - Manage swarm: docker swarm <COMMAND>
* Dockerfile commands (https://docs.docker.com/reference/dockerfile/):
  - ADD: Add local or remote files and directories. Automatically unpacks as a directory if src is a local tar archive in a recognized compression format.
  - FROM: Create a new build stage from a base image. FROM scratch (starts from a special empty base)
  - USER: Set user and group ID.
